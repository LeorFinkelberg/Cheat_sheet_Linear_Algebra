\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
		]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Сборник заметок по линейной алгебре и сопряженным вопросам}

\author{\itshape Подвойский А.О.}

\date{}
\maketitle

\thispagestyle{fancy}


%\shorttableofcontents{Краткое содержание}{1}


\tableofcontents

\section{Ранг матрицы}

\emph{Базисный минор} -- это отличный от нуля минор наивысшего порядка.

\emph{Ранг} матрицы это наибольший порядок отличного от нуля минора этой матрицы. Отличные от нуля миноры наивысшего порядка называют \emph{базисными}. Столбцы матрицы, на которых располагается хотя бы один базисный минор этой матрицы \emph{линейно независимы} и называются \emph{базисными столбцами}.

Если ранг матрицы совпадает с числом ее столбцов $ \text{rg} A_{m \times n} = n $, то все столбцы этой матрицы линейно независимы \cite[\strbook{31}]{shevtsov:linal-2012}. Существеным является то, что ранг матрицы не изменяется при элементарных преобразованиях \cite[\strbook{34}]{shevtsov:linal-2012}.

Ранг матрицы можно вычислить методом окаймляющих миноров \cite{bortakovskiy:2005}, \cite[\strbook{31}]{shevtsov:linal-2012}.

\section{Базис линейного пространства. Координаты векторов}

Всякую систему векторов линейного пространства $ X $ называют \emph{базисом} или \emph{базой этого пространства}, если эта система векторов \emph{линейно независима} и любой вектор пространства $ X $ линейно выражается через векторы этой системы.

Если базис пространства конечен, т.е. состоит из конечного числа векторов, то он представляет собой конечную максимальную линейно независимую систему векторов пространства, и обратно, \emph{любая конечная максимальная линейно независимая система векторов} линейного пространства является \emph{базисом} этого пространства.

Линейное пространство $ X $ называют конечномерным, если оно обладает хотя бы одним базисом, состоящим из конечного числа векторов. Конечномерное пространство может обладать многими различными базисами. Число векторов в каждом базисе конечномерного пространства одинаково. Это число называют \emph{размерностью} пространства и обозначают $ \dim X = n $. Пространство $ X $ при этом называют $ n $-мерным.

То есть размерность пространства $ \dim X $ -- это число векторов в базисе пространства.

Пусть линейное пространство $ X_n $ обладает базисом
\begin{align*}
	e: e_1, e_2, \ldots, e_n.
\end{align*}

Тогда любой вектор $ x $ из $ X_n $ единственным образом представляется в виде
\begin{align*}
	x = x_1 \, e_1 + x_2 \, e_2 + \ldots x_n \, e_n = e \cdot [x]_e.
\end{align*}

Числа $ x_1, x_2, \ldots $ в разложении называют координатами вектора $ x $ в базисе и записывают $ x \ (x_1, x_2, \ldots, x_n)^T_e $. 

Система линейных уравнений, имеющая \emph{хотя бы одно} решение, называется \emph{совместной}. Если у системы \emph{нет решений}, то она называется \emph{несовместной} \cite[\strbook{41}]{shevtsov:linal-2012}.

Система совместна тогда и только тогда, когда ранг матрицы $ A $ системы равен рангу расширенной матрицы $ A | b $: $ \text{rg} A = \text{rg} (A | b) $. Ранг матрицы определяется порядком базисного минора.

Если ранг матрицы совпадает с числом неизвестных и система совместна, то она имеет строго одно решение. Если же ранг матрицы меньше числа неизвестных и матрица совместна, то она имеет бесконечное множество решений.

Система называется \emph{однородной}, если все свободные члены ее уравнения равны нулю. Кроме того, \emph{однородная система всегда совместна}, так как она обладает, по крайней мере, нулвым решением $ x_1 = x_2 = \ldots = x_n = 0 $.

Для того чтобы система линейных \emph{однородных} уравнений имела \emph{только нулевое} решение необходимо и достаточно, чтобы ранг матрицы совпадал с числом неизвестных системы, $ \rg A_{n \times n} = n $ \cite[\strbook{43}]{shevtsov:linal-2012}. В частности, однородная система $ n $ линейных уравнений с $ n $ неизвестными имеет только нулевое решение, если ее определитель отличен от нуля. Если определитель однородной системы отличен от нуля, то это значит, что ранг матрицы совпадает с числом столбцов (неизвестных), а однородная система всегда совместна. В этом случае может существовать только одно решение -- нулевое.

Та же мысль, но немного по-другому. Пусть определитель матрицы коэффициентов $ A_{n \times n} $ \emph{однородной} системы линейных алгебраических уравнений отличен от нуля, $ \det A_{n \times n} \neq 0 $. Это значит, что можно записать
$$
A \, x = 0,\ \rightarrow A^{-1} A \, x = A^{-1} \, 0,\ \rightarrow \underline{x = 0} \ (\text{при} \ \det A \neq 0)
$$

То есть если матрица коэффициентов однородной СЛАУ имеет полный столбцовый ранг (столбцы линейно независимы), то у этой однородной системы может быть только тривиальное нулевое решение.

Для того чтобы \emph{однородная} система имела \emph{ненулевые} решения, необходимо и достаточно, чтобы ранг ее матрицы был меньше числа неизвестных система, $ \text{rg} A < \#x_k $. В частности, \emph{однородная} система $ n $ линейных уравнений с $ n $ неизвестными имеет \emph{ненулевые} решения, когда ее определитель равен нулю, $ \det A_{n \times n} = 0 $. Любая однородная система, в которой число уравнений меньше числа неизвестных имеет ненулвые решения.

Однородная СЛАУ имеет ненулвые решения при нулевом определителе потому, что в этом случае у СЛАУ $ \rg A_{n \times n} < n $, то есть уравнений меньше, чем неизвестных. А это значит, что матрица коэффициентов, например для СЛАУ с тремя неизвестными, выглядит так
\begin{align*}
	A = \begin{pmatrix}
		1 & -3 & 2\\
		4 & -1 & 8 \\
		0 & 0 & 0
	\end{pmatrix}
\end{align*}

Очевидно, что определитель такой матрица равен $ \det A = 0 $. Потому и получается, что если у однородной СЛАУ уравнений меньше, чем неизвестных, то определитель матрицы коэффициентов у нее обнуляется.

Характеристические числа линейного оператора, принадлежащие основному полю $ P $, и только они, являются собственными значениями этого оператора \cite[\strbook{65}]{shevtsov:linal-2012}.

\section{Собственные векторы и собвственные значения линейного оператора}

Ненулевой вектор $ x $ из $ X_n $ называют \emph{собственным вектором оператора} $ \varphi $, если этим оператором он переводится в вектор $ \lambda x $, т.е.
\begin{align*}
	\varphi x = \lambda x,
\end{align*}
где $ \lambda $ -- некоторое число из поля $ P $, называемое \emph{собственным значением оператора} $ \varphi $. При этом говорят, что собственный вектор $ x $ принадлежит собственному значению $ \lambda $.

В матричной форме
\begin{align*}
	A \, X = \lambda X.
\end{align*}

Отсюда получаем
\begin{align*}
	(A - \lambda E) \, X = 0.
\end{align*}

Эта однородная система имеет ненулевые решения с координатами из поля $ P $ только тогда, когда определитель $ \det (A - \lambda E) = 0 $ и $ \lambda $ принадлежит полю $ P $. Но это означает, что $ \lambda $ является корнем характеристического многочлена $ \det (A - \lambda E | $ и принадлежит полю $ P $.

Множество всех собственных значений линейного оператора (каждое собственное значение берется столько раз, какова его кратность в характеристическом многочлене) называют \emph{спектром линейного оператора}. Если матрицу отождествляют с оператором, то множество всех ее собственных значений называют \emph{спектром матрицы}.

То есть для отыскания всех собственных значений оператора с матрицей $ A $ нужно найти все характеристические числа матрицы $ A $ из них выбрать лишь те, которые принадлежат основному полю, а для отыскания всех собственных векторов оператора с матрицей $ A $ нужно найти все ненулевые решения системы $ (A - \lambda E) \, X = 0 $ при каждом собственном значении $ \lambda $ \cite[\strbook{65}]{shevtsov:linal-2012}.

Пример \cite[\strbook{66}]{shevtsov:linal-2012}: Для оператора с матрицей
\begin{align*}
	A = \begin{pmatrix}
		\,
		3 & 1 & -3 \\
		3 & 1 & -1 \\
		2 & -2 & 0
		\,
	\end{pmatrix}
\end{align*}
действующего в действтельном просранстве, найти собственные значения и собственные векторы.

Характеристический многочлен
\begin{align*}
	\det (A - \lambda E) = \det \begin{pmatrix}
		\,
		3 - \lambda & 1 & -3 \\
		3 & 1 - \lambda & -1 \\
		2 & -3 & -\lambda 
		\,
	\end{pmatrix}
    = (\lambda^2 + 4)(4 - \lambda)
\end{align*}
матрицы $ A $ имеет корни $ \lambda_1 = 4, \lambda_{2,3} = \pm 2 i $. Так как рассматриваемый оператор действует в действительном линейном пространстве, то его собственным значением будет только $ \lambda = 4 $. При этом значении $ \lambda $ система $ (A - \lambda E) X = 0 $ имеет вид
\begin{align*}
	- x_1 + x_2 - 3 x_3 = 0, \\
	3 x_1 - 3 x_2 - x_3 = 0, \\
	2 x_1 - 2 x_2 - 4 x_3 = 0.
\end{align*}

Ее общим решением является $ X = (x_1, x_1, 0)^T $ с произвольным постоянным $ x_1 $. При $ x_1 $, пробегающим все действительные значения, оно дает общий вид собственных векторов оператора с матрицей $ A $, принадлежащих собственному значению $ \lambda = 4 $. Других действительных собственных векторов оператор с матрицей $ A $ не имеет, так как у него нет других собственных значений.

Собственные векторы линейного оператора $ \varphi $ с матрицей $ A $, принадлежащие одному и тому же собственному значению $ \lambda $, вместе с нулевым вектором образуют подпространство, которое называют \emph{собственным подпространством оператора} $ \varphi $ \emph{по} $ \lambda $.

Размерность собственного подпространства оператора $ \varphi $ по собственному значению $ \lambda $ равна $ n - r_\lambda $, где $ n $ -- порядок матрицы $ A $, $ r_\lambda $ -- ранг матрицы $ A - \lambda E $. Эту размерность называют \emph{геометрической кратностью собственного значения} $ \lambda $.

Другими словами, геометрической кратностью собственного значения $ \lambda $ (то есть $ n - r_\lambda $) называют \emph{максимальное число линейно независимых собственных векторов} оператора $ \varphi $, принадлежащих собственному значению $ \lambda $ \cite[\strbook{66}]{shevtsov:linal-2012}.

Геометрическая кратность собственного значения не превосходит его алгебраической кратности, т.е. кратности, с которой $ \lambda $ входит корнем в характеристический многочлен $ \det (A - \lambda E) $.

Собственные векторы линейного оператора, принадлежащие различным собственным значениям, линейно независимы. Их линейные комбинации, вообще говоря, не являются собственными векторами оператора $ \varphi $.

Если отождествлять оператор с его матрицей, то естественно говорить о \emph{собственных значениях} и \emph{собственных векторах матрицы}. На практике так обычно и делают.

Квадратную матрицу называют \emph{простой}, если для каждого собственного значения матрицы его геометрическая кратность совпадает с алгебраической кратностью. В противном случае матрицу называют \emph{дефектной}.

\section{Операторы простой структуры}

Для того чтобы квадратная матрица $ A $ с элементами из поля $ P $ была \emph{матрицей простой структуры}, т.е. чтобы она приводилась к \emph{диагональному виду}, необходимо и достаточно \cite[\strbook{69}]{shevtsov:linal-2012}
\begin{itemize}
	\item чтобы все характеристические числа $ \lambda_i $ матрицы $ A $ принадлежали полю $ P $,
	
	\item чтобы геометрическая и алгебраическая кратность каждого числа $ \lambda_i $ совпадали.
\end{itemize}

Квадратная матрица с элементами из поля $ P $ ($ A_{n \times n}, a_{ij} \in P $), все характеристические числа которой различны и принадлежат полю $ P $ ($ \lambda_i \neq \lambda_j, i \neq j, \lambda_k \in P $), приводится к \emph{диагональному виду}.

Соотношение 
\begin{align*}
	A = T \Lambda T^{-1}
\end{align*}
называют \emph{каноническим} или \emph{спектральным} разложением матрицы $ A $ \cite[\strbook{69}]{shevtsov:linal-2012}. Таким образом, матрица прострой структуры, т.е. приводитмая к диагональному виду имеет спектральное разложение.

\section{Евклидовы пространства}

Действительное линейное $ n $-мерное пространство, в котором определено скалярное умножение векторов, называют $ n $-мерным евклидовым пространством и обозначают $ E_n $.

Скалярное произведение векторов $ x $ и $ y $ можно представить так
\begin{align*}
	(x, y) = x^T \Gamma y,
\end{align*}
где $ x = (x_1, x_2, \ldots, x_n) $, $ y = (y_1, y_2, \ldots, y_n) $,
\begin{align*}
	\Gamma = \begin{pmatrix}
		(e_1, e_1) & \ldots & (e_1, e_n) \\
		\ldots & \ldots & \ldots \\
		(e_n, e_1) & \ldots & (e_n, e_1)
	\end{pmatrix}
\end{align*}

Матрицу $ \Gamma $ называют \emph{матрицей Грама базиса} $ e_1, e_2, \ldots, e_n $. Матрица Грама симметрическая, так как $ (a_i, a_j) = (a_j, a_i) $. Определитель матрицы Грама любой \emph{линейно независимой} системы векторов положителен ($ \det \Gamma > 0$), а \emph{линейно зависимой} системы векторов равен нулю ($ \det \Gamma = 0 $) \cite[\strbook{72}]{shevtsov:linal-2012}.

Для задания в линейном пространстве $ X_n $ скалярного произведения векторов при фиксированном базисе  $ e_1, e_2, \ldots, e_n $ нужно взять в качестве матрицы $ \Gamma $ какую-либо симметрическую матрицу порядка $ n $ с положительными главными диагональными минорами (\emph{положительно определенную симметрическую матрицу}). 

Например, матрицу Грамма можно положить равной единичной $ \Gamma = E $, а можно и другими способами, но каждый раз матрица Грамма должна быть симметрической и положительно определенной (то есть все собственные значения матрицы должны быть положительны).

Всякая ортогональная система ненулевых векторов \emph{линейно независима} \cite[\strbook{74}]{shevtsov:linal-2012}.

Квадратная матрица $ Q $, для которой транспонированная  матрица $ Q^T $ совпадает с обратной матрицей $ Q^{-1} $ ($ Q^{-1} = Q^T $), называется \emph{ортогональной матрицей}. Квадратная матрица $ Q $ является ортогональной, если и только если $ Q^T Q = Q Q^T = E $ \cite[\strbook{79}]{shevtsov:linal-2012}.

Основные свойства ортогональной матрицы \cite[\strbook{79}]{shevtsov:linal-2012}:
\begin{itemize}
	\item Квадратная матрица $ Q $ ортогональная тогда и только тогда, когда сумма квадратов всех элементов любого ее столбца (строки) равна единице, а сумма попарных произведений соответсвующих элементов двух любых столбцов (строк) равна нулю.
	
	\item Определитель ортогональной матрицы равен $ \pm 1 $, $ \det Q = \pm 1 $.
	
	\item Матрица, обратная к ортогональной матрице, тоже ортогональая.
	
	\item Произведение ортогональных матриц является ортогональной матрицей.
\end{itemize}

\emph{Симметрическая} матрица $ A $ называется \emph{неотрицательной} (\emph{положительно определенной}), если для любого вектора $ x \neq 0 $ выполняется условие $ x^T A x \geqslant 0 $ ($ x^T A x > 0 $).

\emph{Симметрическая} матрица является неотрицательной (положительно определенной) тогда и только тогда, когда все ее \emph{характеристические числа}\footnote{В книге Гудфеллоу Я. \cite[\strbook{54}]{goodfellow:ml-2018} вместо \emph{характеристических чисел} используются \emph{собственные значения} и \emph{неотрицательная} матрица называется \emph{положительно полуопределенной}} неотрицательные (положительные) \cite[\strbook{92}]{shevtsov:linal-2012}.







\section{Мера обусловленности матрицы}

\emph{Мера} (или \emph{число}) \emph{обусловленности матрицы} $ A $ определяется как \cite[\strbook{306}]{bahvalov:num_methods}
\begin{align*}
	\nu(A) = \| A \| \, \| A^{-1} \|
\end{align*}
 
Поскольку \emph{\underline{любая} норма} матрицы не меньше своего наибольшего по модулю собственного значения, то $ \| A \| \geqslant \max | \lambda_A | $ и поскольку собственные значения матриц $ A $ и $ A^{-1} $ взаимо обратны, то
\begin{align*}
	\| A^{-1} \| \geqslant \max \dfrac{1}{ | \lambda_A | } = \dfrac{1}{ \min | \lambda_A | }
\end{align*}

Таким образом, мера обусловленности матрицы $ A $
\begin{align*}
	\boxed{\nu(A) \geqslant \dfrac{\max | \lambda_A |}{ \min | \lambda_A | } \geqslant 1}
\end{align*}

В частности, при $ A = A^T $ (то есть если матрица симметричная) имеем $ \| A \|_2 = \max | \lambda_A | $.

Следовательно, в случае нормы $ \| \cdot \|_2 $
\begin{align*}
	\nu(A) = \dfrac{ \max | \lambda_A | }{ \min | \lambda_A | }.
\end{align*}


\section{Линейно зависимые и линейно независимые системы}

Система из $ k $ столбцов $ A_1, \ldots, A_k $ называется \emph{линейно зависимой}, если существуют такие числа $ \alpha_1, \ldots, \alpha_k $ не все равные нулю одновременно, что \cite[\strbook{128}]{bortakovskiy:2005}
\begin{align*}
	\alpha_1 A_1 + \alpha_2 A_2 + \ldots \alpha_k A_k = o,
\end{align*}
где $ o $ -- нулевой вектор соответствующего размера.

То есть другими словами система $ k $ столбцов называется \emph{линейно зависимой}, если эти столбцы \emph{суммируются в нулевой столбец} для нетривиального случая коэффициентов $ \alpha_i $ (когда эти коэффициенты не все одновременно равны нулю).

Система из $ k $ столбцов называется \emph{линейно независимой}, если $ \sum\limits_{j=1}^k \alpha_j A_j = o $ возможно только в тривиальном случае, т.е. когда $ \alpha_1 = \alpha_2 = \ldots = \alpha_k = 0 $.

\subsection{Свойства линейно зависимых и линейно независимых столбцов}

\remark{
	Понятия линейной зависимости и линейной независимости формулируются одинаково как для строк, и так для столбцов
}

\begin{itemize}
	\item Если в систему входит \emph{нулевой столбец}, то она \emph{линейно зависима},
	
	\item Если в систему входит \emph{два равных столбца}, то она \emph{линейно зависима},
	
	\item Если в системе столбцов имеется два пропорциональных столбца $ A_i = \lambda A_j $, то она линейно зависима,
	
	\item Любые столбцы, входящие в \emph{линейно независимую систему}, образуют \emph{линейно независимую подсистему},
	
	\item Система столбцов, содержащая \emph{линейно зависимую подсистему}, сама \emph{линейно зависима},
	
	\item Если система столбцов $ A_1, \ldots,A_k $ -- линейно независима, а после присоединения к ней столбца $ A $ -- оказывается линейно зависимой, то столбец $ A $ можно разложить по столбцам $ A_1, \ldots, A_k $ и притом единственным образом, т.е. коэффициенты определяются однозначно.
\end{itemize}

\section{Система $ m $ линейных алгебраических уравнений с $ n $ неизвестными}

Матричная запись неоднородной системы уравнений имеет вид
\vspace*{-3mm}
\begin{align*}
	A x = b,
\end{align*}

\vspace*{-3mm}
а однородной
\vspace*{-3mm}
\begin{align*}
	A x = o,
\end{align*}

\vspace*{-3mm}
где $ o $ в правой части обозначает нулевой столбец размеров $ m \times 1 $.

Эту матричную запись неоднородной системы уравнений можно представить в эквивалентной форме
\begin{align*}
	\begin{pmatrix}
		a_{11} \\
		\vdots \\
		a_{m1}
	\end{pmatrix} x_1 + 
    \begin{pmatrix}
    	a_{12} \\
    	\vdots \\
    	a_{m2}
    \end{pmatrix} x_2 + \ldots +
    \begin{pmatrix}
    	a_{1n} \\
    	\vdots \\
    	a_{mn}
    \end{pmatrix} x_n = 
    \begin{pmatrix}
    	b_1 \\
    	\vdots \\
    	b_m.
    \end{pmatrix}
\end{align*}

Тогда решение системы представляется столбцом
\begin{align*}
	x =
	\begin{pmatrix}
		\alpha_1 \\
		\vdots \\
		\alpha_n
	\end{pmatrix}
\end{align*}
и удовлетворяте равенству
\begin{align*}
	\begin{pmatrix}
		a_{11} \\
		\vdots \\
		a_{m1}
	\end{pmatrix} \alpha_1 +
    \begin{pmatrix}
    	a_{12} \\
    	\vdots \\
    	a_{m2}
    \end{pmatrix} \alpha_2 + \ldots + 
    \begin{pmatrix}
    	a_{1n} \\
    	\vdots \\
    	a_{mn}
    \end{pmatrix} \alpha_n =
    \begin{pmatrix}
    	b_1 \\
    	\vdots \\
    	b_m,
    \end{pmatrix}
\end{align*}
т.е. столбец свободных членов $ b $ является линейной комбинацией столбцов матрицы системы.




\section{Теорема (правило) Крамера}

Система называется \textbf{совместной}, если она имеет \emph{хотя бы одно решение}. Система называется \textbf{несовместной}, если она \emph{не имеет ни одного решения}.

Если определитель $ \Delta = \det A $ матрицы системы $ n $ линейный независимых уравнений с $ n $ неизвестными отличен от нуля ($ \det A \neq 0 $), то система имеет \emph{единственное} решение, которое находится по формулам
\begin{align*}
	x_i = \dfrac{ \Delta_i }{ \Delta }, \ i = 1,\ldots, n, \quad (\Delta = \det A \neq 0),
\end{align*}
где $ \Delta_i $ -- определитель матрицы, полученной из матрицы системы $ A = [a_{ij}]_{i,j=1}^n $ заменой $ i $-ого столбца столбцом свободных членов.

ЗАМЕЧАНИЕ: на практике при больших $ n $ правило Крамера не применяется!

Если $ \Delta = 0 $ (матрица коэффициентов системы вырождена) и хотя бы один определитель $ \Delta_i \neq 0 $, то система \emph{несовместна}, т.е. не имеет ни одного решения. Если же $ \Delta = \Delta_1 = \Delta_2 = \ldots, \Delta_n = 0 $, то возможны два случая: либо система несовместна (не имеет ни одного решения), либо система имеет бесконечно много решений \cite[\strbook{188}]{bortakovskiy:2005}.

\section{Условие совместности системы линейных уравнений. Теорема Кронекера-Капелли}

Рассмотрим систему $ m $ линейных уравнений с $ n $ неизвестными. Составим блочную матрицу, приписав к матрице $ A $ справа столбец свободных членов $ b $. Получим \emph{расширенную матрицу системы}
\begin{align*}
	\underset{m \times (n + 1)}{(  A \ | \ b )} =
	  \begin{pmatrix}
	  	  a_{11} & \dots & a_{1n} &  b_1\\
	  	  a_{21} & \dots & a_{2n}  & b_2 \\
	  	  \vdots  & \ddots & \vdots & \vdots \\
	  	  a_{m1} & \dots & a_{mn} & b_m
	  \end{pmatrix}
\end{align*}

Эта матрица содержит всю информацию о системе уравнений, за исключением обозначений неизвестных.

\emph{Теорема Кронекера-Капелли}. Система $ A x = b $ \emph{совместна} (т.е. имеет хотя бы одно решение) тогда и только тогда, когда ранг матрицы системы равен рангу расширенной матрицы $ \rg A = \rg (A \ | \ b) $.

Если $ \rg A \neq \rg (A \ | \ b) $, то система несовместна -- не имеет решений.

Если система имеет решение, то столбец свободных членов есть линейная комбинация столбцов матрицы системы. Поэтому при вычеркивании столбца $ b $ из расширенной матрицы $ (A \ | \ b) $ ее ранг не изменяется. Следовательно, $ \rg (A \ | \ b) = \rg A $.

ЗАМЕЧАНИЕ: теорема Кронекера-Капелли дает лишь критерий существования решения системы, но не указывает способа отыскать этого решения.


\section{Общее решение системы линейных алгебраических уравнений}

Неизвестные, которым соответствуют столбцы, входящие в базисный минор, называются \emph{базисными переменными}, остальные неизвестные -- \emph{свободными переменными}.

\emph{Общее решение} системы, выржающее базисные переменные через свободные, имеет вид \cite[\strbook{192}]{bortakovskiy:2005}
\begin{align*}
	\begin{cases}
		x_1 = b_1^{'} - a_{1, r + 1}^{'} x_{r + 1} - \ldots - a_{1, n}^{'} x_n, \\
		\ldots \\
		x_r = b_r^{'} - a_{r, r + 1}^{'} x_{r + 1} - \ldots - a_{r, n}^{'} x_n,
	\end{cases}
\end{align*}
где $ x_1, x_2, \ldots, x_r $ -- базисные переменные; $ x_{r + 1}, x_{r + 2}, \ldots, x_{n} $ -- свободные переменные.

\emph{Частное решение} системы -- решение системы, получающееся из общего решения, заданием конкретных значений свободными переменным.

Пусть $ x^{н} $ -- решение неоднородной системы. Тогда любое решение $ x $ неоднородной системы можно представить в виде $ x = x^\text{н} + x^\text{о} $, где $ x^\text{о} $ -- решение однородной системы.

Говорят, что \emph{общее решение} неоднородной системы есть сумма \emph{частного решения} \underline{неоднородной} системы и \emph{общего решения} соответствующей \underline{однородной} системы \cite[\strbook{200}]{bortakovskiy:2005}
\begin{align*}
	x = x^\text{н} + C_1 \varphi_1 + C_2 \varphi_2 + \ldots + C_{n - r} \varphi_{n - r}.
\end{align*}


\section{Решение систем уравнений с помощью полуобратных матриц}

Требуется решить систему линейных уравнений
\begin{align*}
	A x = b,
\end{align*}
где $ A $ -- \underline{произвольная} матрица размера $ m \times n $.

Если матрица системы нулевая $ A = O $, то система либо несовместна (при $ b = o $), либо имеет бесконечное множество решений (при $ b = o $ любой подходящий по размерам столбец $ x $ является решением). Далее рассматривается случай ненулевой матрицы $ A $.

Пусть $ A^{\neg 1} $ -- матрица, полуобратная к матрице системы $ A $. Используя определение полуобратной матрицы, неоднородную систему $ Ax = b $ можно переписать так
\begin{align*}
	A A^{\neg 1} A x = b.
\end{align*}

Если $ x $ -- решение системы, то подставляя $ A x = b $ в левую часть последнего соотношения
\begin{align*}
	A A^{\neg 1} A x = b, \quad \rightarrow \quad A A^{\neg 1} b = b.
\end{align*}

Тогда
\begin{align*}
  (E_m - A A^{\neg 1}) \, b = o.
\end{align*}

Это необходимое и достаточное условие совместности системы.

Решением системы будет $ x = A^{\neg 1} b $. Но поскольку \emph{полуобратная матрица} определена \emph{неоднозначно}, то эта формула фактически задает множество решений системы. Преобразуем так, чтобы была видна структура этого множества, в частности, выявим количество независимых параметров
\begin{align*}
  A^{\neg 1}_0 = T \Lambda^T S = T \,
    \begin{pmatrix}
          \begin{array}{c | c}
          	E_r & O \\
          	\hline
          	O & O
          \end{array}
    \end{pmatrix}
    S,
\end{align*}
где $ S $ и $ T $ -- элементарные матрицы порядков $ n $ и $ m $ соответственно, $ \Lambda $ -- матрица простейшего вида, эквивалентная матрице $ A $ ($ \Lambda \sim A $), $ \rg A $.

\emph{Теорема о совместности неоднородной системы и о структуре ее общего решения}. Неоднородная система $ A x = b $ \emph{совместна} тогда и только тогда, когда столбец свободных членов является решением однородной системы $ \Psi b = o $. Если система $ A x = b $ совместна, то ее общее решение имеет вид \cite[\strbook{205}]{bortakovskiy:2005}
\begin{align*}
  x = x^\text{н} + x^\text{о} = A_0^{\neg 1} \, b + \Psi \, c = T
  \begin{pmatrix}
  \begin{array}{c | c}
    E_r & O \\
    \hline 
    O & O
  \end{array}
  \end{pmatrix}
  S \, b + T
  \begin{pmatrix}
    \begin{array}{c}
    	O \\
    	\hline
    	E_{n - r}
    \end{array}
  \end{pmatrix}
  c, \quad
  \Psi =
  \begin{pmatrix}
  	\begin{array}{c | c}
  		O & E_{m - r} S
  	\end{array}
  \end{pmatrix},
\end{align*}
где $ T, S $ -- элементарные преобразующие матрицы, $ c = (C_1 \ldots C_{n - r})^T $ -- столбец произвольных постоянных.

Алгоритм применения полуобратной матрицы:
\begin{enumerate}
	\item Привести матрицу $ A $ системы $ A x = b $ к простейшему виду: $ \Lambda = S A T $. При этом находятся элементраные преобразующие матрицы $ S $ и $ T $, а также ранг $ r = \rg A \geqslant 1 $.
	
	\item Проверить условие совместности системы $ \Psi b = o $. При $ r = m $ система совместна. Если $ r < m $, то составить матрицу $ \Psi = (O \, | \, E_{m - r}) \, S $ и проверить условие $ \Psi b = o $. Если условие выполняется, то система совместна. В противном случае система несовместна и процесс решения заканчивается.
	
	\item Найти частное решение неоднородной системы по формуле
	\begin{align*}
      x^\text{н} = A_o^{\neg 1} \, b = T
        \begin{pmatrix}
        	\begin{array}{c | c}
        		E_r & O \\
        		\hline
        		O & O
        	\end{array}
        \end{pmatrix}
        S \, b
	\end{align*}

    \item Составить фундаментальную матрицу
    \begin{align*}
    	\Phi = T 
    	\begin{pmatrix}
    		\begin{array}{c}
    			O \\
    			\hline
    			E_{n - r}.
    		\end{array}
    	\end{pmatrix}
    \end{align*}

    \item Записать общее решение системы в виде
    \begin{align*}
    	x = x^\text{н} + \Phi \, c,
    \end{align*}
    где $ c = (C_1 \ldots C_{n - r})^T $ -- столбец произвольных постоянных.
\end{enumerate}


\section{Псевдорешения системы линейных уравнений}

Система $ m $ линейных алгебраических уравнений с $ n $ неизвестными $ A x = b $ может иметь единственное решение, бесконечно много решений или вообще не иметь решений. Нужно изменить понятие решения так, чтобы любая система линейных уравнений имела бы единственное в некотором смысле <<решение>>.

Поставим каждому столбцу в соответсвие неотрицательное действительное число, а именно норму (модуль)
$$
| x | = \Big(\sum\limits_{i=1}^{n} x_i^2 \Big)^{1/2}.
$$

\emph{Псевдорешением} системы линейных уравнений называется наименьший по норме столбец $ \tilde{x} $ среди всех столбцов, минимизирующих величину $ |A x - b | $.

ЗАМЕЧАНИЕ: \emph{любая} система имеет единственное псевдорешение \cite[\strbook{209}]{bortakovskiy:2005}
\begin{align*}
	\tilde{x} = A^{\sim 1} b,
\end{align*}
где $ A^{\sim 1} $ -- псевдообратная матрица для матрицы системы.

Понятие псевдорешения позволяет обойти не только факт неединственности, но и факт несуществования решений.

Если система несовместна, то псевдорешение $ \tilde{x} $ обеспечивает наименьшую величину погрешности $ \varepsilon(x) = | A x - b | $.

Если система совместна, то псевдорешение $ \tilde{x} $ является ее решением, т.е. $ \varepsilon(\tilde{x}) = 0 $, причем наименьшим по норме.

Алгоритм нахождения псевдорешения неоднородной системы:
\begin{enumerate}
	\item Найти псевдообратную матрицу $ A^{\sim 1} $.
	
	\item Найти псевдорешение $ \tilde{x} = A^{\sim 1} b $.
\end{enumerate}

ЗАМЕЧАНИЕ: \emph{полуобратная} матрица определена \underline{неоднозначно} и потому задает не конкретное решение, а \emph{множество решений} системы. \emph{Псевдорешение}, полученное с помощью псевдообратной матрицы, всегда вычисляется в \emph{конкретное решение}.


\section{Свойства решений однородной системы}

Общее решение однородной системы $ Ax = o $ имеет вид \cite[\strbook{194}]{bortakovskiy:2005}
\begin{align*}
	\begin{cases}
		x_1 = -a_{1, r + 1}^{'} x_{r + 1} - \ldots - a_{1,n}^{'}x_n,\\
		{\centering \ldots} \\
		x_r = -a_{r, r + 1}^{'} x_{r + 1} - \ldots - a_{r,n}^{'} x_n.
	\end{cases}
\end{align*}

Некоторые свойства:
\begin{itemize}
	\item Если столбцы $ \varphi_1, \varphi_2, \ldots, \varphi_k $ -- решения однородной системы уравнений, то любая их линейная комбинация $ \alpha_1 \, \varphi_1 + \alpha_2 \, \varphi_2 + \ldots + \alpha_k \, \varphi_k $ также является решением однородной системы,
	
	\item Если ранг матрицы однородной системы равен $ r $, то система имеет $ (n - r) $ \emph{линейно независимых решений}.
\end{itemize}

Любая совокупность $ (n - r) $ линейно независимых решений $ \varphi_1, \varphi_2, \ldots, \varphi_{n - r} $ однородной системы называется \emph{фундаментальной системой решений}.

\emph{Теорема об общем решении однородной системы}. Если $ \varphi_1, \varphi_2, \ldots, \varphi_{n - r} $ -- фундаментальная система решений однородной системы уравнений, то столбец
\begin{align}\label{eq:ordinsys}
	x = C_1 \varphi_1 + C_2 \varphi_2 + \ldots + C_{n - r} \varphi_{n - r}
\end{align}
при любых значениях произвольных постоянных $ C_1, C_2, \ldots, C_{n - r} $ также является решением системы $ A x = o $, и, наоборот, для каждого решения $ x $ этой системы найдутся такие значения произвольных постоянных $ C_1, C_2, \ldots, C_{n - r} $, при которых это решение $ x $ удовлетворяет равенству \eqref{eq:ordinsys}.

\section{Функциональные матрицы скалярного аргумента}

\emph{Функциональной матрицей скалярного аргумента} $ t $ называется матрица, элементы которой являются функциями независимой переменной $ t $
\begin{align*}
	\underset{m \times n}{A(t)} = [\, a_{ij}(t) \,]_{i,j=1}^{m,n}
\end{align*}

Производная функциональной матрицы
\begin{align*}
	\underset{m \times n}{\dfrac{d A(t)}{dt}} = \Big[ \dfrac{d a_{ij}(t) }{dt} \Big]_{i,j=1}^{m, n}.
\end{align*}

Производная обратной матрицы (если она существует)
\begin{align*}
	\underset{m \times n}{ \dfrac{d A^{-1}(t)}{dt} } = - A^{-1}(t) \,\dfrac{d A(t)}{dt} \, A^{-1}(t).
\end{align*}

Производная определителя квадратной матрицы $ A(t) $ $ n $-ого порядка
\begin{align*}
	\dfrac{d}{dt} \det A(t) = \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij}(t) \dfrac{d a_{ij}(t)}{dt} = \tr \Bigg[ A^{+}(t) \, \dfrac{d A(t)}{dt} \Bigg],
\end{align*}
где $ A_{ij}(t) $ -- алгебраическое дополнение элемента $ a_{ij}(t) $ матрицы $ A(t) $; $ A^{+}(t) $ -- присоединенная матрица.


\section{Производные скалярной функции по векторному аргументу}

Рассмотрим скалярную (числовую) функцию нескольких переменных $ f(x_1, x_2, \ldots, x_n) $. Упорядоченный набор переменных $ x_1, x_2, \ldots, x_n $ будем называть \emph{векторным аргументом} этой функции.

\emph{Первый дифференциал} функции $ f(x) = f(x_1, x_2, \ldots, x_n) $ имеет вид
\begin{align*}
	df(x) = \dfrac{ \partial f(x) }{ \partial x_1 } dx_1 + \dfrac{ \partial f(x) }{ \partial x_2 } dx_2 + \ldots + \dfrac{ \partial f(x) }{ \partial x_n } dx_n.
\end{align*}

Сумму в правой части можно представить как произведение строки $ \dfrac{ df(x) }{dx} = \Big( \dfrac{\partial f(x)}{\partial x_1} \ldots \dfrac{\partial f(x)}{\partial x_n} \Big) $ на столбец $ dx = (dx_1 \ldots dx_n)^T $, либо как произведение строки $ dx^T $ на столбец $ dx = \dfrac{ df(x) }{dx^T} = \Big( \dfrac{df(x)}{dx} \Big)^T $.

Так как первый дифференциал $ \underset{1 \times 1}{df(x)} $ -- это одноэлементная матрица (а одноэлементая матрица совпадает со своей транспонированной), то $ \underset{1 \times 1}{df(x)} = \big(\underset{1 \times 1}{df(x)}\big)^T $
\begin{align*}
	\underset{1 \times 1}{df(x)} = \underset{1 \times n}{\dfrac{ df(x) }{ dx }} \, 
	\underset{n \times 1}{dx} = \Big( \dfrac{ df(x) }{dx} dx \Big)^T = dx^T \Big(\dfrac{df(x)}{dx}\Big)^T = \underset{1 \times n}{dx}^T \, \underset{n \times 1}{\dfrac{ df(x) }{dx^T}}.
\end{align*}

\emph{Второй дифференциал} функции имеет вид
\begin{align*}
	d^2f(x) = \sum_{i=1}^{n}\sum_{j=1}^{n} \dfrac{ \partial^2 f(x) }{ \partial x_i \partial x_j } dx_i dx_j.
\end{align*}

Обозначим через $ \dfrac{d^2 f(x)}{dx^T dx} = \Big[ \dfrac{ \partial^2 f(x) }{ \partial x_i^2 \partial x_j^2 } \Big]_{i,j=1}^{n} $ квадратную матрицу частных производных второго порядка (\emph{матрицу Гессе}). Определитель матрицы Гессе называется \emph{гессианом}.

Тогда можно переписать
\begin{align*}
	\underset{1 \times 1}{d^2 f(x)} = \underset{1 \times n}{dx}^T \, \underset{n \times n}{\dfrac{ d^2 f(x) }{ dx^T dx }} \, \underset{n \times 1}{dx}.
\end{align*}

Для скалярной функции скалярного аргумента второй дифференциал будет иметь вид
\begin{align*}
	d^2 f(x) = \dfrac{d^2f(x)}{dx^2}dx^2.
\end{align*}

Для записи производных можно использовать символические векторы (столбцы или строки)
\begin{align*}
	\nabla = \dfrac{d}{dx} = \Big( \dfrac{\partial}{\partial x_1} \ldots \dfrac{\partial}{\partial x_n} \Big), \quad \nabla^T = \dfrac{d}{dx^T} =
	\begin{pmatrix}
		\dfrac{\partial}{\partial x_1} \\
		\vdots\\
		\dfrac{\partial}{\partial x_n}
	\end{pmatrix}.
\end{align*}

При этом дифференциирование функции формально записывается как как умножение функции на символический вектор производных. Например, градиент функции есть произведение вектора $ \nabla $ на функцию $ f(x) $
\begin{align*}
	\nabla f(x_1, \ldots, x_n) = \Big( \dfrac{ \partial f }{ \partial x_1 } \ldots \dfrac{ \partial f }{ \partial x_n }  \Big), \\
	\underset{n \times 1}{\nabla}^T \underset{1 \times n }{\nabla} =
	\begin{pmatrix}
		\dfrac{ \partial }{ \partial x_1 }\\
		\ldots \\
		\dfrac{ \partial }{ \partial x_n }
	\end{pmatrix}
    \Big( \dfrac{\partial}{\partial x_1} \ldots \dfrac{\partial}{\partial x_n} \Big) = \Bigg[ \dfrac{\partial^2}{\partial x_i \partial x_j} \Bigg]_{i, j=1}^n.
\end{align*}

Найти первую и вторую производные сложной функции $ g(t) = f(x_1(t), \ldots, x_n(t)) $, применяя матричные обозначения.

Находим производные функции, заменяя суммирование операциями умножения соответствующих матриц. Первая производная
\begin{align*}
	\dfrac{ d g(t) }{ dt } = \dfrac{d}{dt} \Big( f(x_1(t), \ldots, x_n(t))) \Big) = \sum_{i=1}^{n} \dfrac{\partial f(x(t))}{\partial x_i} \cdot \dfrac{ d x_i(t) }{dt} = \dfrac{df(x(t))}{dx} \cdot \dfrac{dx(t)}{dt}
\end{align*}

В случае скалярной функции скалярного аргумента первая производная от функции $ g(x) $ будет выглядеть так же.

Вторая производная скалярной функции векторного аргумента
\begin{multline*}
  \dfrac{d^2 g(t)}{dt^2} = \sum_{j=1}^{n} \sum_{i=1}^{n} \dfrac{ \partial^2 f(x(t)) }{ \partial x_j \partial x_i } \cdot \dfrac{ d x_i(t) }{ dt } \cdot \dfrac{ d x_j(t) }{dt} + \sum_{i=1}^n \dfrac{ \partial f(x(t)) }{\partial x_i} \cdot \dfrac{d x_i^2}{d t^2} = \ldots \\
  \ldots = \Big( \dfrac{d x(t)}{dt} \Big)^T \dfrac{ d^2 f(x(t)) }{ dx^T dx } \cdot \dfrac{dx(t)}{dt} + \dfrac{df(x(t))}{dx} \cdot \dfrac{d^2 x(t)}{dt^2}.
\end{multline*}

В случае скалярной функции скалярного аргумента вторая производная будет выглядеть так
\begin{align*}
  \dfrac{d^2 g(t)}{d t^2} = \dfrac{ d^2 f(x(t)) }{dx^2} \Big( \dfrac{dx(t)}{dt} \Big)^2 + \dfrac{ df(x(t)) }{dx} \cdot \dfrac{ d^2 x(t) }{dt^2}.
\end{align*}

Выражения для первой производной совпадают, а для второй производной -- отличаются незначительно, причем полное совпадение будет, если учесть, что $ x^T = x $ для \emph{скалярной} величины $ x $.

\section{Производные от векторной функции векторного аргумента}

Пусть задан столбец
$$
f(x) =
\begin{pmatrix}
	f_1(x_1, \ldots, x_n)\\
	\vdots \\
	f_n(x_1, \ldots, x_n)
\end{pmatrix}
$$
функций нескольких переменных (говорят, что задана \emph{вектор-функция векторного аргумента}).

Первый дифференциал вектор-функции имеет вид
\begin{align*}
	df(x) =
	\begin{pmatrix}
		df_1(x_1, \ldots, x_n) \\
		\vdots \\
		df_m(x_1, \ldots, x_n)
	\end{pmatrix}
	=
	\begin{pmatrix}
		\sum\limits_{j=1}^{n} \dfrac{\partial f_1(x_1, \ldots, x_n)}{\partial x_j} \, dx_j\\
		\vdots \\
		\sum\limits_{j=1}^{n} \dfrac{\partial f_m(x_1, \ldots, x_n)}{\partial x_j} \, dx_j
	\end{pmatrix} = \sum_{j=1}^{n}
    \begin{pmatrix}
    	\dfrac{\partial f_1(x)}{\partial x_j} \\
    	\vdots \\
    	\dfrac{\partial f_m(x)}{\partial x_j} 
    \end{pmatrix}
    dx_j
\end{align*}

Обозначим через
\begin{align*}
	\dfrac{df(x)}{dx} =
	\begin{pmatrix}
		\dfrac{\partial f_1(x)}{\partial x_1} & \ldots & \dfrac{\partial f_1(x)}{\partial x_n} \\
		\vdots & \ddots & \vdots \\
		\dfrac{\partial f_m(x)}{\partial x_1} & \ldots & \dfrac{\partial f_m(x)}{\partial x_n}
	\end{pmatrix} =
    \Bigg[ \dfrac{\partial f_i(x)}{\partial x_j} \Bigg]_{i,j=1}^{m, n}
\end{align*}
матрицу частных производных первого порядка заданных функций (\emph{матрицу Якоби}).

Тогда выражение для первого дифференциала можно записать в виде $ df(x) = \dfrac{df(x)}{dx} dx $, т.е. $ \dfrac{df(x)}{dx} $ -- производная вектор-функции векторного аргумента.

Как и в случае с аргументом $ x $, упорядоченный набор функций можно считать не матрицей-столбцом, а матрицей-строкой $ (f(x))^T $. Этот случай сводится к предыдущему, учитывая, что операции дифференциирования и транспонирования можно выполнять в любом порядке, так как $ d(f^T) = (df)^T $. Тогда из равенства $ df = \dfrac{df}{dx}dx $ получаем $ df^T = (dx)^T \Big( \dfrac{df}{dx} \Big)^T = (dx)^T \dfrac{df^T}{dx^T} $, где
\begin{align*}
	\Big( \dfrac{df(x)}{dx}\Big)^T = \dfrac{df^T}{dx^T} =
	\begin{pmatrix}
		\dfrac{\partial f_1(t)}{\partial x_1} & \ldots & \dfrac{\partial f_m(x)}{\partial x_1} \\
		\vdots & \ddots & \vdots \\
		\dfrac{\partial f_1(x)}{\partial x_n} & \ldots & \dfrac{\partial f_m(x)}{\partial x_n}
	\end{pmatrix}
\end{align*}
-- транспонированная матрица Якоби вектор-функции векторного аргумента.

\section{Правила дифференциирования по векторному аргументу}

Векторный аргумент $ x $, его приращение $ dx $ считаем вектор-столбцами размеров $ n \times 1 $.

Первый дифференциал скалярной функции векторного аргумента $ f(x_1, \ldots, x_n) $ (одноэлементная матрица) имеет вид
\begin{align*}
	df = \dfrac{df}{dx} \, dx = dx^T \, \dfrac{df}{dx^T},
\end{align*}
где $ \dfrac{df}{dx} = \Big( \dfrac{df}{dx_1} \ldots \dfrac{df}{dx_n} \Big) $ -- градиент функции, а $ \Big( \dfrac{df}{dx} \Big)^T = \dfrac{df}{dx^T} $, так как функция скалярная.

Второй дифференциал скалярной функции векторного аргумента $ f(x_1, \ldots, x_n) $
\begin{align*}
	d^2 f = dx^T \dfrac{d^2 f}{dx^T dx} dx,
\end{align*}
где $ \dfrac{d^2 f}{dx^T dx} = \Bigg[ \dfrac{\partial^2 f(x)}{\partial x_i \partial x_j} \Bigg]_{i,j=1}^{n} $ -- матрица Гессе.

Первый дифференциал вектор-функции векторного аргумента (вектора-столбца) $ f(x) $ имеет вид
\begin{align*}
	df = \dfrac{df(x)}{dx} dx,
\end{align*}
где $ \dfrac{df(x)}{dx} $ -- матрица Якоби.

Первый дифференциал вектора-строки
\begin{align*}
	(df)^T = d(f^T) = dx^T \dfrac{df^T}{dx^T}.
\end{align*}

В частном случае, когда $ f(x_1, \ldots, x_n) = (x_1, \ldots, x_n) $, получаем
\begin{align*}
	\dfrac{dx}{dx} = E, \quad \dfrac{dx^T}{dx^T} = E,
\end{align*}
где $ E $ -- единичная матрица $ n $-ого порядка.

Числовую матрицу $ C $ соответствующих размеров можно выносить за знак производной
\begin{align*}
	\dfrac{d(Cf)}{dx} = C \, \dfrac{df}{dx}, \quad \dfrac{d(f^T C)}{dx^T} = \dfrac{df^T}{dx^T} \, C
\end{align*}

Производные суммы, разности и произведения вектор-функций векторного аргумента $ u(x) $ и $ v(x) $ одинаковых размеров $ m \times 1 $
\begin{align*}
	\dfrac{d(u + v)}{dx} = \dfrac{du}{dx} + \dfrac{dv}{dx}, \quad \dfrac{d(u - v)}{dx} = \dfrac{du}{dx} - \dfrac{dv}{dx}, \\
	\dfrac{d(u^T v)}{dx} = u^T \dfrac{dv}{dx} + v^T \dfrac{du}{dx}, \quad \dfrac{d(u^T v)}{dx^T} = \dfrac{du^T}{dx^T} v + \dfrac{d v^T}{dx^T} u.
\end{align*}

Производная сложной функции $ z(y(x)) $, где $
z = z(y) = 
\begin{pmatrix}
	z_1(y) \\
	\vdots \\
	z_k(y)
\end{pmatrix}
$ и $
y = y(x) = 
\begin{pmatrix}
	y_1(x) \\
	\vdots \\
	y_m(x)
\end{pmatrix}
$, вычисляется по формуле $ \dfrac{dz(y(x))}{dx} = \dfrac{dz(y(x))}{dy} \, \dfrac{dy(x)}{dx} $ или, опуская аргументы, $ \dfrac{dz}{dx} = \underset{k \times m}{\dfrac{dz}{dy}} \, \underset{m \times n}{\dfrac{dy}{dx}} $.

След матрицы Якоби (при $ m = n $) определяет \emph{дивергенцию}
\begin{align*}
	\divv f = \tr \dfrac{df}{dx} = \sum_{i=1}^n \dfrac{\partial f_i}{\partial x_i},
\end{align*}
где $ f(x) $ -- векторная функция векторного аргумента.

\section{Производные матричной функции по векторному аргументу}

Рассмотрим функциональную матрицу $ A(x) $, элементами которой служат скалярные функции $ a_{ij}(x) $ векторного аргумента $ x $. То есть такая матрица представляет собой трехмерную сущность, в которой на пересечении строки и столбца стоит скалярная функция, имеющая <<глубину>> в виде вектора аргументов.

Первый дифференциал этой функции
\begin{align*}
	d A(x) = \sum_{i=1}^n \dfrac{\partial A(x)}{\partial x_i} dx_i,
\end{align*}
где $ \dfrac{\partial A(x)}{\partial x_i} $ -- частная производная матрицы по одной переменной.

Совокупность частных производных (градиент функциональной матрицы) представляет собой объект, элементы которого $ \dfrac{\partial a_{ij}(x)}{\partial x_k} $ нумеруются тремя индексами: номер строки, номер столбца и номер переменной дифференциирования. Поэтому заменить операцию суммирования в правой части формулы операцией умножения матриц в данном случае не представляется возможным. Необходимо вводить тензоры и операции над ними.

Элементы матрицы $ A = (a_j^i) $ обозначаются $ a_j^i $, где $ i $ -- номер строки, а $ j $ -- номер столбца. В частности, $ x = (x^i) $ -- столбец, а $ y = (y_j) $ -- строка.

Частную производную функции $ F(x) $ (склалярной, векторной или матричной), то по ним производится суммирование (хотя знак суммы не указывается). Например, если $ A = (a_j^i) $ -- матрица размеров $ m \times n $, $ x = (x^j) $ -- столбец размеров $ n \times 1 $, $ y = (y_i) $ -- строка размеров $ 1 \times m $, то
\begin{align*}
	a_j^i x^j = \sum_{j=1}^n a_j^i x^j, \quad a_j^i y_i = \sum_{i=1}^m a_j^i y_i, \quad a_j^ix^jy_i = \sum_{i=1}^m\sum_{j=1}^n a_j^i x^jy_i,
\end{align*}
т.е. $ a_j^i x^j $ -- $ i $-ый элемент столбца $ A x $; $ a_j^i y_i $ -- $ j $-ый элемент строки $ yA $; $ a_j^i x^jy_i $ -- число $ yAx $.

Применяя эти соглашения, запишем дифференциалы
\begin{align*}
	df = f_{(i)}dx^i, \quad d^2 f = f_{(i)(j)}dx^idx^j,\\
	df^i = f_{(j)}^i dx^j, \\
	df_j^i = f_{j(k)}^i dx^k,
\end{align*}
где $ f_{j(k)}^i = \dfrac{\partial f_j^i}{\partial x^k} $ -- частная производная первого порядка элемента $ f_j^i $ функциональной матрицы $ F $ по переменной $ x^k $.

\section{Линейные и квадратичные формы}

Многочлен первой степени от $ n $ переменных $ x_1, \ldots, x_n $ называется выражением вида
\begin{align*}
	p_1(x) = c_1 x_1 + c_2 x_2 + \ldots + c_n x_n + c_0,
\end{align*}
где $ c_0, c_1, \ldots, c_n $ -- коэффициенты многочлена (предполагается, что среди коэффициентов есть отличные от нуля); коэффициент $ c_0 $ называется свободным членом. Многочлен перовой степени называется однородным, если $ p_1(\lambda x) = \lambda p_1 (x) $ для любого числа $ \lambda $ (это возможно только когда $ c_0 = 0 $).

\emph{Линейной формой} переменных $ x_1, \ldots, x_n $ называется однородный многочлен первой степени
\begin{align*}
	g(x) = \sum_{i=1}^n c_i x_i,
\end{align*}
где $ \{c_i\}_{i=1}^n $ -- коэффициенты линейной формы. 

Составляя из коэффициентов строку $ c = (c_1 \ldots c_n) $, а из переменных -- столбец $ x = (x_1 \ldots x_n)^T $, линейную форму можно записать в виде
\begin{align*}
	g(x) = cx.
\end{align*}

Многочлен второй степени от $ n $ переменных $ x_1, \ldots, x_n $ называется выражение
$$
p_2(x) = \sum_{i=1}^n\sum_{j=1}^m a_{ij}x_ix_j + \sum_{i=1}^n b_i x_i + c_0,
$$
где числа $ a_{ij}, b_i, c_0 $ -- коэффициенты многочлена: $ a_{ij} $ -- страшие коэффициенты; $ b_i $ -- коэффициенты линейных членов; $ c_0 $ -- свободный член.

Многочлен второй степени называется однородным, если $ p_2(\lambda x) = \lambda^2 p_2(x) $ (это возможно только когда $ b_1 = b_2 = \ldots = b_n = 0, c_0 = 0 $).

\emph{Квадратичной формой} переменных $ x_1, \ldots, x_n $ называется однородный многочлен второй степени
\begin{align*}
	q(x) = \sum_{i=1}^n \sum_{j=1}^n a_{ij}x_i x_j, \quad a_{ij} = a_{ji}.
\end{align*}

Симметрическая матрица $ A = (a_{ij}) $, составленная из коэффициентов квадратичной формы, называется \emph{матрицей квадратичной формы}.

Квадратичная форма называется \emph{вырожденной}, если ее матрица вырожденая ($ \rg A < n $), в противном случае, когда матрица невырожденная ($ \rg A = n $), квадратичная форма называется \emph{невырожденной}.

Составляя из переменных столбец $ x = (x_1 \ldots x_n)^T $, квадратичную форму можно записать в виде
\begin{align*}
	q(x) = x^T A x.
\end{align*}




% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	\bibitem{bortakovskiy:2005}{\emph{Бортаковский А.С.} Линейная алгебра в примерах и задачах. -- М.: Высш. шк., 2005. -- 591~с.}
	
	\bibitem{goodfellow:ml-2018}{\emph{Гудфеллоу Я.} Глубокое обучение, 2018. -- 652 с. }
	
	\bibitem{bahvalov:num_methods}{\emph{Бахвалов Н. С.} Численные методы. -- М.: Лаборатория Базовых Знаний, 2000.~--624 ~с.}
	
	\bibitem{shevtsov:linal-2012}{\emph{Шевцов Г.С.} Численные методы линейной алгебры. -- М.: Финансы и статистика, 2012. -- 480~с.}
\end{thebibliography}

\end{document}
